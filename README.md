# Toxicity-Checker-Abuse-Language-Detection
This project provides a Toxicity Checker that detects abusive or harmful language in a given text using a Hugging Face Model. The application interacts with the Hugging Face API to evaluate the text for toxicity and provide a report with scores and labels for toxicity. It is a full-stack web application built with Node.js, Express, and CSS .
